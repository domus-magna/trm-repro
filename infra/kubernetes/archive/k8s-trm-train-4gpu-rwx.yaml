apiVersion: batch/v1
kind: Job
metadata:
  name: trm-train-arc2-4gpu
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      nodeSelector:
        gpu.nvidia.com/model: H200
      containers:
        - name: trainer
          image: pytorch/pytorch:2.5.0-cuda12.4-cudnn9-runtime
          imagePullPolicy: IfNotPresent
          env:
            - name: WANDB_API_KEY
              valueFrom:
                secretKeyRef:
                  name: wandb-api-key
                  key: token
            - name: WANDB_MODE
              value: online
            - name: WANDB_PROJECT
              value: trm-arc2
          command: ["bash","-lc"]
          args:
            - |
              set -euo pipefail
              nvidia-smi || true
              apt-get update -y >/dev/null && apt-get install -y --no-install-recommends git ca-certificates >/dev/null && rm -rf /var/lib/apt/lists/*
              python3 -V
              pip install --upgrade pip wheel setuptools >/dev/null
              # Prepare TRM repo without clobbering existing data/
              if [ -d /workspace/TinyRecursiveModels/.git ]; then
                cd /workspace/TinyRecursiveModels && git pull --ff-only || (git fetch origin main && git reset --hard FETCH_HEAD)
              else
                mkdir -p /workspace/TinyRecursiveModels
                cd /workspace/TinyRecursiveModels
                git init
                git remote add origin https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git || true
                git fetch --depth=1 origin main
                git reset --hard FETCH_HEAD
              fi
              # Install deps (torch preinstalled)
              pip install -r requirements.txt >/dev/null || true
              pip install --no-cache-dir --no-build-isolation adam-atan2 wandb >/dev/null
              # Verify dataset exists on RWX PVC
              [ -d data/arc2concept-aug-1000 ] || { echo 'Dataset missing on RWX PVC'; ls -la data || true; exit 2; }
              # Run training (faithful 4-GPU)
              run_name="trm_arc2_paper_repro"
              torchrun --nproc-per-node 4 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \
                arch=trm \
                data_paths="[data/arc2concept-aug-1000]" \
                arch.L_layers=2 \
                arch.H_cycles=3 arch.L_cycles=4 \
                +run_name=${run_name} ema=True
          volumeMounts:
            - name: trm-workspace
              mountPath: /workspace
          resources:
            limits:
              nvidia.com/gpu: 4
              cpu: "8"
              memory: 48Gi
            requests:
              nvidia.com/gpu: 4
              cpu: "4"
              memory: 24Gi
      volumes:
        - name: trm-workspace
          persistentVolumeClaim:
            claimName: trm-workspace-rwx
