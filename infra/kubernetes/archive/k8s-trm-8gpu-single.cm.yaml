apiVersion: batch/v1
kind: Job
metadata:
  name: trm-train-arc2-8gpu
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      tolerations:
        - key: is_gpu
          operator: Equal
          value: "true"
          effect: PreferNoSchedule
      nodeSelector:
        kubernetes.io/hostname: gd90782
      containers:
        - name: trainer
          image: nvidia/cuda:12.1.0-devel-ubuntu22.04
          imagePullPolicy: IfNotPresent
          env:
            - name: WANDB_API_KEY
              valueFrom:
                secretKeyRef:
                  name: wandb-api-key
                  key: token
            - name: WANDB_MODE
              value: online
            - name: WANDB_PROJECT
              value: trm-arc2
            - name: CUDA_HOME
              value: /usr/local/cuda
            - name: DISABLE_COMPILE
              value: "1"
            - name: HYDRA_FULL_ERROR
              value: "1"
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: PYTHONPATH
              value: /usr/local/lib/python3.10/dist-packages:/workspace/TinyRecursiveModels:/workspace
          command: ["bash","-lc"]
          args:
            - |
              . /script/common.sh
              cd /workspace/TinyRecursiveModels
              export PATH="/root/.local/bin:${PATH}"
              python3 -m precheck
              run_name="trm_arc2_8gpu_eval100"
              resume_ckpt="checkpoints/Arc2concept-aug-1000-ACT-torch/${run_name}/step_62976"
              export RESUME_STEP="${resume_ckpt##*_}"
              python3 -m torch.distributed.run --nproc_per_node 8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \
                arch=trm \
                data_paths="[data/arc2concept-aug-1000]" \
                arch.L_layers=2 \
                arch.H_cycles=3 arch.L_cycles=4 \
                +run_name=${run_name} ema=True \
                checkpoint_every_eval=True \
                epochs=10000 eval_interval=100 \
                +load_checkpoint="${resume_ckpt}"
          volumeMounts:
            - name: trm-workspace
              mountPath: /workspace
            - name: script
              mountPath: /script/common.sh
              subPath: common.sh
            - name: trm-eval-overlay
              mountPath: /workspace/TinyRecursiveModels/evaluators/arc.py
              subPath: arc.py
            - name: trm-pyshim
              mountPath: /usr/local/lib/python3.10/dist-packages
            - name: dshm
              mountPath: /dev/shm
          resources:
            limits:
              nvidia.com/gpu: 8
              cpu: "16"
              memory: 96Gi
            requests:
              nvidia.com/gpu: 8
              cpu: "8"
              memory: 48Gi
      volumes:
        - name: trm-workspace
          persistentVolumeClaim:
            claimName: trm-workspace-rwx
        - name: script
          configMap:
            name: trm-common-script
        - name: trm-eval-overlay
          configMap:
            name: trm-eval-overlay
        - name: trm-pyshim
          configMap:
            name: trm-pyshim-cm
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
