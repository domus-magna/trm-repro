---
alwaysApply: true
description: Step-by-step ARC-AGI-2 reproduction commands and guardrails
---

## Steps â€” Paper-Faithful TRM on ARC-AGI-2

### 1) System sanity & folders
```bash
set -euo pipefail
command -v git >/dev/null || { echo "git not found"; exit 1; }
command -v "$PY" >/dev/null || { echo "Python 3.10 not found (PY=$PY)"; exit 1; }
command -v pip >/dev/null || { echo "pip not found"; exit 1; }

mkdir -p "$TRM_DIR" ".repro" "artifacts" "logs"
if command -v nvidia-smi >/dev/null 2>&1; then nvidia-smi -L || true; fi
```

### 2) Clone upstream TRM (no modifications)
```bash
if [ ! -d "$TRM_DIR/.git" ]; then
  git clone https://github.com/SamsungSAILMontreal/TinyRecursiveModels.git "$TRM_DIR"
else
  (cd "$TRM_DIR" && git pull --ff-only)
fi
(cd "$TRM_DIR" && git rev-parse --short HEAD) | tee ".repro/trm_commit.txt"
```

### 3) Python env & dependencies
```bash
if [ "${USE_VENV:-1}" = "1" ]; then
  "$PY" -m venv "$VENV_DIR"
  . "$VENV_DIR/bin/activate"
fi
pip install --upgrade pip wheel setuptools
pip install --pre --upgrade torch torchvision torchaudio --index-url "$TORCH_INDEX_URL"
pip install -r "$TRM_DIR/requirements.txt"
pip install --no-cache-dir --no-build-isolation adam-atan2
if [ -n "${WANDB_API_KEY}" ]; then
  pip install wandb
  WANDB_API_KEY="$WANDB_API_KEY" wandb login
fi
{ python -V; pip list; python - <<'PY'
import torch, sys
print("torch:", torch.__version__)
print("cuda_available:", torch.cuda.is_available())
PY
} | tee ".repro/env_${TARGET_DATASET}_${RUN_NAME}.txt"
```

### 4) Build ARC-AGI-2 dataset (exact path)
```bash
cd "$TRM_DIR"
if [ ! -d "data/arc2concept-aug-1000" ]; then
  python -m dataset.build_arc_dataset \
    --input-file-prefix kaggle/combined/arc-agi \
    --output-dir data/arc2concept-aug-1000 \
    --subsets training2 evaluation2 concept \
    --test-set-name evaluation2
fi
[ -d "data/arc2concept-aug-1000" ] || { echo "ARC-AGI-2 dataset missing"; exit 2; }
find "data/arc2concept-aug-1000" -maxdepth 2 -type f | head -n 8
```

### 5) Train TRM on ARC-AGI-2 (paper-faithful)
```bash
cd "$TRM_DIR"
NGPUS=$( (nvidia-smi -L 2>/dev/null | wc -l) || echo 0 )
echo "GPUs detected: ${NGPUS}"
if [ "${NGPUS:-0}" -ge 4 ]; then
  torchrun --nproc-per-node "${MAX_GPUS_TO_USE}" \
    --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 \
    pretrain.py \
      arch=trm \
      data_paths="[data/arc2concept-aug-1000]" \
      arch.L_layers=2 \
      arch.H_cycles=3 arch.L_cycles=4 \
      +run_name="${RUN_NAME}" ema=True
else
  python pretrain.py \
    arch=trm \
    data_paths="[data/arc2concept-aug-1000]" \
    arch.L_layers=2 \
    arch.H_cycles=3 arch.L_cycles=4 \
    +run_name="${RUN_NAME}" ema=True
fi
```

### 6) Verify artifacts & provenance
```bash
RUN_DIR="$TRM_DIR/runs/$RUN_NAME"
[ -d "$RUN_DIR" ] || { echo "Run folder missing: $RUN_DIR"; exit 3; }
CKPT=$(ls -t "$RUN_DIR"/checkpoints/* 2>/dev/null | head -n1 || true)
if [ -z "$CKPT" ]; then
  CKPT=$(ls -t "$RUN_DIR"/*.ckpt 2>/dev/null | head -n1 || true)
fi
[ -n "$CKPT" ] || { echo "No checkpoint found in $RUN_DIR"; exit 4; }
(cd "$TRM_DIR" && git rev-parse HEAD) > "$RUN_DIR/TRM_COMMIT.txt"
echo "pretrain.py arch=trm data_paths=[data/arc2concept-aug-1000] arch.L_layers=2 arch.H_cycles=3 arch.L_cycles=4 +run_name=$RUN_NAME ema=True" \
  > "$RUN_DIR/COMMANDS.txt"
printf "RUN_NAME=%s\nCKPT=%s\nTRM_COMMIT=%s\n" \
  "$RUN_NAME" "$CKPT" "$(cat "$RUN_DIR/TRM_COMMIT.txt")" \
  | tee ".repro/summary_${RUN_NAME}.txt"
mkdir -p artifacts
printf "%s\n" \
  "RUN_DIR=$RUN_DIR" \
  "CKPT=$CKPT" \
  "ENV_LOG=.repro/env_${TARGET_DATASET}_${RUN_NAME}.txt" \
  "SUMMARY=.repro/summary_${RUN_NAME}.txt" \
  > artifacts/MANIFEST.txt
```

### Guardrails
- Do **not** alter batch size, learning rate, weight decay, precision, or EMA unless upstream says so.
- If single-GPU OOM: stop and report; do not change flags.
- If crash mid-run and resume is supported upstream, use their documented resume behavior; otherwise re-start.
- Do not mix ARC-AGI-1 inputs with ARC-AGI-2.